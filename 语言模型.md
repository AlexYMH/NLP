## 语言模型

### 一，数据平滑方法

#### （一）加一法

**核心思想：**每一种情况出现的次数加1。

#### （二）减值法/折扣法

**核心思想：**修改训练样本中事件的实际计数，使样本中(实际出现的)不同事件的概率之和小于1，剩余的概率量分配给未见事件的概率。

#### 1，Good-Turing估计

首先总数为$N=\sum_{r=1}^{\infty}n_rr$

由于$N=\sum_{r=0}^{\infty}n_rr^*=\sum_{r=0}^{\infty}(r+1)n_{r+1}$，则：
$$
r^*=(r+1)\frac{n_{r+1}}{n_r}
$$
则其概率值为：
$$
p_r=\frac{r^*}{N}
$$
有$\frac{n_1}{N}$的剩余概率量均分给所有的未见事件。

**Good-Turing估计适用于大词汇集产生的符合多项式分布的大量的观察数据。**

因为修改后的$\sum_{r=0}^{\infty}p_r\ne 1$，所以需要进行归一化处理：
$$
\hat{p}_r=\frac{p_r}{\sum_{r}p_r}
$$

#### 2，Back-off(后备/后退)法

**又称Katz后退法**。

**基本思想：**当某一事件在样本中出现的频率大于阈值K(K=0或1)时，运用**最大似然估计的减值法**来估计其概率，否则，使用低阶的，即$(n-1)gram$的概率替代$n-gram$概率，而这种替代需受归一化因子$\alpha$的作用。

**另一种解释：**对于每个计数 $r > 0$ 的n元文法的出现次数减值, 把因减值而节省下来的剩余概率根据低阶的 (n-1)gram 分 配给未见事件。

 **详见我的博客：[语言模型和平滑方法](https://blog.csdn.net/h2026966427/article/details/79811252)**

#### 3，绝对减值法

**核心思想：**从每个计数 r 中减去同样的量，剩余的概率量由未见事件均分。 

#### 4，线性减值法

**核心思想：**从从每个计数 r 中减去与该计数成正比 的量(减值函数为线性的)，剩余概率量$\alpha$被$n_0$个未 见事件均分。 即：
$$
\begin{equation}
p_r=\left\{
             \begin{array}{lr}
             \dfrac{(1-\alpha)r}{N}& \quad r>0  \\
             \dfrac{\alpha}{n_0}&\quad r=0\\
             \end{array}
\right.
\end{equation}
$$
自由参数$\alpha$的优化值为：$\dfrac{n_1}{N}$

**绝对减值法产生的n-gram通常优于线性减值法。**

#### 5，四种减值法的比较

- **Good-Turing法：**对非0事件按公式削减出现的次数，节留出来的概率均分给0概率事件。
- **Katz后退法：**对非0事件按Good-Turing法计算减值，节留出来的概率按低阶分布分给0概率事件。
- **绝对减值法：**对非0事件无条件削减某一固定的出现次数值，节留出来的概率均分给0概率事件。
- **线性减值法：**对非0事件根据出现次数按比例削减次数值，节留出来的概率均分给0概率事件。

#### （三）删除插值法（线性插值法）

**核心思想：**用低阶语法估计高阶语法，即当 3-gram 的值不能从训练数据中准确估计时，用 2-gram 来替代，同样，当 2-gram 的值不能从训练语料中准确估计时，可以用 1-gram 的值来代替。 、

插值公式如下：
$$
p(w_3|w_1w_2)=\lambda_3p^{'}(w_3|w_1w_2)+\lambda_2p^{'}(w_3|w_2)+\lambda_1p^{'}(w_3)
$$
其中，$\lambda_1+\lambda_2+\lambda_3=1$

### 二，语言模型的自适应

**问题：**

- 语言模型对于训练文本的类型、主题和风格等都十分敏感; 
- n 元语言模型的独立性假设的前提是一个文 本中的当前词出现的概率只与它前面相邻的 n-1个词相关，但这种假设在很多情况下是明显不成立的。 

**自适应方法：**

- **基于缓存的语言模型**

  **核心思想：**语言模型根据n-gram的线性插值求得。

- **基于混合方法的语言模型**

  **核心思想：**将语言模型划分成 n 个子模型 M1, M2, ..., Mn，整个语言模型的概率通过线性插值公式计算得到:
  $$
  \hat{p}(w_i|w_1^{i-1})=\sum_{j=1}^{n}\lambda_j \hat{p}_{M_j}(w_i|w_1^{i-1})
  $$
  其中，$0\le\lambda_j\le 1$，$\sum_{j=1}^n\lambda_j=1$ 。

  $\lambda$通过EM算法求得。

- **基于最大熵的语言模型**

  **核心思想：**通过结合不同信息源的信息构建一个语言 模型。每个信息源提供一组关于模型参数的约束条件，在所有满足约束的模型中，选择熵最大的模型。 

